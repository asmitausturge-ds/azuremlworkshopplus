{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "98605bcd",
      "metadata": {},
      "source": [
        "# Responsible AI GUI\n",
        "\n",
        "## Analysis of Synthetic Data\n",
        "\n",
        "This demo illustrates a hypothetical scenario of how likely a programmer should be given access to a GPT2 model for inferencing, based on information such as their favorite programming language, preference for tabs vs spaces, OS, location and so forth. Each programmer will be given a score between [0,10] where a score between [7,10] indicates access given to the programmer and [0,7) indicates access denied. The data were synthetically generated via the [PyPI package, Fibber.io](https://pypi.org/project/fibber/).\n",
        "\n",
        "First, we need to specify the version of the RAI components which are available in the workspace. This was specified when the components were uploaded, and will have defaulted to '1':"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7968ddd-cd1a-4d74-9119-dab1ab3aa95e",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ">[NOTE] Must use Python 3.10 SDK V2 for this demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b4eeac",
      "metadata": {
        "gather": {
          "logged": 1671089210775
        }
      },
      "outputs": [],
      "source": [
        "version_string = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06008690",
      "metadata": {},
      "source": [
        "We also need to give the name of the compute cluster we want to use in AzureML. Later in this notebook, we will create it if it does not already exist:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ad79f9",
      "metadata": {
        "gather": {
          "logged": 1671089211089
        }
      },
      "outputs": [],
      "source": [
        "compute_name = \"cpu-cluster\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc65dc7",
      "metadata": {},
      "source": [
        "Finally, we need to specify a version for the data and components we will create while running this notebook. This should be unique for the workspace, but the specific value doesn't matter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78053935",
      "metadata": {
        "gather": {
          "logged": 1671089211422
        }
      },
      "outputs": [],
      "source": [
        "rai_programmer_example_version_string = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c15429",
      "metadata": {},
      "source": [
        "## Configure workspace details and get a handle to the workspace\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the MLClient from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10627d86",
      "metadata": {
        "gather": {
          "logged": 1671089212266
        }
      },
      "outputs": [],
      "source": [
        "# Enter details of your AML workspace\n",
        "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
        "resource_group = \"<RESOURCE_GROUP>\"\n",
        "workspace = \"<AML_WORKSPACE_NAME>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d33e34",
      "metadata": {
        "gather": {
          "logged": 1671089213410
        }
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "# ml_client = MLClient(\n",
        "#     credential=credential,\n",
        "#     subscription_id=subscription_id,\n",
        "#     resource_group_name=resource_group,\n",
        "#     workspace_name=workspace,\n",
        "# )\n",
        "\n",
        "ml_client = MLClient.from_config(\n",
        "    credential=credential,\n",
        ")\n",
        "print(ml_client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54738095",
      "metadata": {
        "gather": {
          "logged": 1671089213730
        }
      },
      "outputs": [],
      "source": [
        "# Get handle to azureml registry for the RAI built in components\n",
        "registry_name = \"azureml\"\n",
        "ml_client_registry = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=ml_client.subscription_id,\n",
        "    resource_group_name=ml_client.resource_group_name,\n",
        "    registry_name=registry_name,\n",
        ")\n",
        "print(ml_client_registry)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73be2b63",
      "metadata": {},
      "source": [
        "## Accessing the Data\n",
        "\n",
        "We supply the synthetic data as a pair of parquet files and accompanying `MLTable` file. We can read them in and take a brief look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f875f18",
      "metadata": {
        "gather": {
          "logged": 1671089214147
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d53df4",
      "metadata": {},
      "source": [
        "Now define the paths to the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7bbe58",
      "metadata": {
        "gather": {
          "logged": 1671089214343
        }
      },
      "outputs": [],
      "source": [
        "train_data_path = \"data/programmer-rai-data/train/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c4eb082",
      "metadata": {
        "gather": {
          "logged": 1671089214679
        }
      },
      "outputs": [],
      "source": [
        "test_data_path = \"data/programmer-rai-data/test/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c4ebb4",
      "metadata": {},
      "source": [
        "Load some data for a quick view:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1027fa92",
      "metadata": {
        "gather": {
          "logged": 1671089215329
        }
      },
      "outputs": [],
      "source": [
        "import mltable\n",
        "\n",
        "tbl = mltable.load(train_data_path)\n",
        "train_df: pd.DataFrame = tbl.to_pandas_dataframe()\n",
        "\n",
        "# test dataset should have less than 5000 rows\n",
        "test_df = mltable.load(test_data_path).to_pandas_dataframe()\n",
        "assert len(test_df.index) <= 5000\n",
        "\n",
        "display(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1115ac59",
      "metadata": {},
      "source": [
        "The (synthetic) data are about a collection of programmers, with a 'score' column which we wish to predict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b42df3d",
      "metadata": {
        "gather": {
          "logged": 1671089215548
        }
      },
      "outputs": [],
      "source": [
        "target_column_name = \"score\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e79b04",
      "metadata": {},
      "source": [
        "First, we need to upload the datasets to our workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62eb02a2",
      "metadata": {
        "gather": {
          "logged": 1671089218379
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "input_train_data = \"Programmers_Train_MLTable\"\n",
        "input_test_data = \"Programmers_Test_MLTable\"\n",
        "\n",
        "try:\n",
        "    # Try getting data already registered in workspace\n",
        "    train_data = ml_client.data.get(\n",
        "        name=input_train_data, version=rai_programmer_example_version_string\n",
        "    )\n",
        "    test_data = ml_client.data.get(\n",
        "        name=input_test_data, version=rai_programmer_example_version_string\n",
        "    )\n",
        "except Exception as e:\n",
        "    # If no data of specified version exist, create new one\n",
        "    train_data = Data(\n",
        "        path=train_data_path,\n",
        "        type=AssetTypes.MLTABLE,\n",
        "        description=\"RAI programmers training data\",\n",
        "        name=input_train_data,\n",
        "        version=rai_programmer_example_version_string,\n",
        "    )\n",
        "    ml_client.data.create_or_update(train_data)\n",
        "\n",
        "    test_data = Data(\n",
        "        path=test_data_path,\n",
        "        type=AssetTypes.MLTABLE,\n",
        "        description=\"RAI programmers test data\",\n",
        "        name=input_test_data,\n",
        "        version=rai_programmer_example_version_string,\n",
        "    )\n",
        "    ml_client.data.create_or_update(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6815ba75",
      "metadata": {},
      "source": [
        "# Creating the Model\n",
        "\n",
        "To simplify the model creation process, we're going to use a pipeline.\n",
        "\n",
        "We create a directory for the training script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e78d869b",
      "metadata": {
        "gather": {
          "logged": 1671089218640
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"register_model_src\", exist_ok=True)\n",
        "os.makedirs(\"programmer_component_src\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea86e55d",
      "metadata": {},
      "source": [
        "Next, we write out our training script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a523f144",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile programmer_component_src/training_script_reg.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "\n",
        "from azureml.core import Run\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "import mltable\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
        "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
        "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "def create_regression_pipeline(X, y):\n",
        "    pipe_cfg = {\n",
        "        'num_cols': X.dtypes[X.dtypes == 'int64'].index.values.tolist(),\n",
        "        'cat_cols': X.dtypes[X.dtypes == 'object'].index.values.tolist(),\n",
        "    }\n",
        "    num_pipe = Pipeline([\n",
        "        ('num_imputer', SimpleImputer(strategy='median')),\n",
        "        ('num_scaler', StandardScaler())\n",
        "    ])\n",
        "    cat_pipe = Pipeline([\n",
        "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
        "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "    ])\n",
        "    feat_pipe = ColumnTransformer([\n",
        "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
        "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
        "    ])\n",
        "\n",
        "    # Append classifier to preprocessing pipeline.\n",
        "    # Now we have a full prediction pipeline.\n",
        "    pipeline = Pipeline(steps=[('preprocessor', feat_pipe),\n",
        "                               ('model', LinearRegression())])\n",
        "    return pipeline.fit(X, y)\n",
        "\n",
        "def main(args):\n",
        "    current_experiment = Run.get_context().experiment\n",
        "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
        "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
        "    mlflow.set_tracking_uri(tracking_uri)\n",
        "    mlflow.set_experiment(current_experiment.name)\n",
        "    \n",
        "    # Read in data\n",
        "    print(\"Reading data\")\n",
        "    tbl = mltable.load(args.training_data)\n",
        "    all_data = tbl.to_pandas_dataframe()\n",
        "\n",
        "    print(\"Extracting X_train, y_train\")\n",
        "    print(\"all_data cols: {0}\".format(all_data.columns))\n",
        "    y_train = all_data[args.target_column_name]\n",
        "    X_train = all_data.drop(labels=args.target_column_name, axis=\"columns\")\n",
        "    print(\"X_train cols: {0}\".format(X_train.columns))\n",
        "\n",
        "    print(\"Training model\")\n",
        "    # The estimator can be changed to suit\n",
        "    model = create_regression_pipeline(X_train, y_train)\n",
        "\n",
        "    # Saving model with mlflow - leave this section unchanged\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        print(\"Saving model with MLFlow to temporary directory\")\n",
        "        tmp_output_dir = os.path.join(td, \"my_model_dir\")\n",
        "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
        "\n",
        "        print(\"Copying MLFlow model to output path\")\n",
        "        for file_name in os.listdir(tmp_output_dir):\n",
        "            print(\"  Copying: \", file_name)\n",
        "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
        "            # an option, removing the need for listdir\n",
        "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
        "\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65af35fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile register_model_src/register.py\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "from azureml.core import Run\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Based on example:\n",
        "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-cli\n",
        "# which references\n",
        "# https://github.com/Azure/azureml-examples/tree/main/cli/jobs/train/lightgbm/iris\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--model_input_path\", type=str, help=\"Path to input model\")\n",
        "    parser.add_argument(\n",
        "        \"--model_info_output_path\", type=str, help=\"Path to write model info JSON\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_base_name\", type=str, help=\"Name of the registered model\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_suffix\", type=int, help=\"Set negative to use epoch_secs\"\n",
        "    )\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    current_experiment = Run.get_context().experiment\n",
        "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
        "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
        "    mlflow.set_tracking_uri(tracking_uri)\n",
        "    mlflow.set_experiment(current_experiment.name)\n",
        "\n",
        "    print(\"Loading model\")\n",
        "    mlflow_model = mlflow.sklearn.load_model(args.model_input_path)\n",
        "\n",
        "    if args.model_name_suffix < 0:\n",
        "        suffix = int(time.time())\n",
        "    else:\n",
        "        suffix = args.model_name_suffix\n",
        "    registered_name = \"{0}_{1}\".format(args.model_base_name, suffix)\n",
        "    print(f\"Registering model as {registered_name}\")\n",
        "\n",
        "    print(\"Registering via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=mlflow_model,\n",
        "        registered_model_name=registered_name,\n",
        "        artifact_path=registered_name,\n",
        "    )\n",
        "\n",
        "    print(\"Writing JSON\")\n",
        "    dict = {\"id\": \"{0}:1\".format(registered_name)}\n",
        "    output_path = os.path.join(args.model_info_output_path, \"model_info.json\")\n",
        "    with open(output_path, \"w\") as of:\n",
        "        json.dump(dict, fp=of)\n",
        "\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e115dd6e",
      "metadata": {},
      "source": [
        "Now, we can build this into an AzureML component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d54e43f",
      "metadata": {
        "gather": {
          "logged": 1671089219346
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "\n",
        "yaml_contents = f\"\"\"\n",
        "$schema: http://azureml/sdk-2-0/CommandComponent.json\n",
        "name: rai_programmers_training_component\n",
        "display_name: Programmers training component for RAI example\n",
        "version: {rai_programmer_example_version_string}\n",
        "type: command\n",
        "inputs:\n",
        "  training_data:\n",
        "    type: path\n",
        "  target_column_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model_output:\n",
        "    type: path\n",
        "code: ./programmer_component_src/\n",
        "environment: azureml://registries/azureml/environments/AzureML-responsibleai-0.20-ubuntu20.04-py38-cpu/versions/4\n",
        "command: >-\n",
        "  python training_script_reg.py\n",
        "  --training_data ${{{{inputs.training_data}}}}\n",
        "  --target_column_name ${{{{inputs.target_column_name}}}}\n",
        "  --model_output ${{{{outputs.model_output}}}}\n",
        "\"\"\"\n",
        "\n",
        "yaml_filename = \"ProgrammersRegTrainingComp.yaml\"\n",
        "\n",
        "with open(yaml_filename, \"w\") as f:\n",
        "    f.write(yaml_contents)\n",
        "\n",
        "train_model_component = load_component(source=yaml_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9cde33c",
      "metadata": {
        "gather": {
          "logged": 1671089219611
        }
      },
      "outputs": [],
      "source": [
        "yaml_contents = f\"\"\"\n",
        "$schema: http://azureml/sdk-2-0/CommandComponent.json\n",
        "name: register_model\n",
        "display_name: Register Model\n",
        "version: {rai_programmer_example_version_string}\n",
        "type: command\n",
        "is_deterministic: False\n",
        "inputs:\n",
        "  model_input_path:\n",
        "    type: path\n",
        "  model_base_name:\n",
        "    type: string\n",
        "  model_name_suffix: # Set negative to use epoch_secs\n",
        "    type: integer\n",
        "    default: -1\n",
        "outputs:\n",
        "  model_info_output_path:\n",
        "    type: path\n",
        "code: ./register_model_src/\n",
        "environment: azureml://registries/azureml/environments/AzureML-responsibleai-0.20-ubuntu20.04-py38-cpu/versions/4\n",
        "command: >-\n",
        "  python register.py\n",
        "  --model_input_path ${{{{inputs.model_input_path}}}}\n",
        "  --model_base_name ${{{{inputs.model_base_name}}}}\n",
        "  --model_name_suffix ${{{{inputs.model_name_suffix}}}}\n",
        "  --model_info_output_path ${{{{outputs.model_info_output_path}}}}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "yaml_filename = \"register.yaml\"\n",
        "\n",
        "with open(yaml_filename, \"w\") as f:\n",
        "    f.write(yaml_contents)\n",
        "\n",
        "register_component = load_component(source=yaml_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d165e2b",
      "metadata": {},
      "source": [
        "We need a compute target on which to run our jobs. The following checks whether the compute specified above is present; if not, then the compute target is created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e40fc38",
      "metadata": {
        "gather": {
          "logged": 1671089219924
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
        "\n",
        "if compute_name in all_compute_names:\n",
        "    print(f\"Found existing compute: {compute_name}\")\n",
        "else:\n",
        "    my_compute = AmlCompute(\n",
        "        name=compute_name,\n",
        "        size=\"Standard_D2_v2\",\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "        idle_time_before_scale_down=3600,\n",
        "    )\n",
        "    ml_client.compute.begin_create_or_update(my_compute)\n",
        "    print(\"Initiated compute creation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d8eb868",
      "metadata": {},
      "source": [
        "## Running a training pipeline\n",
        "\n",
        "Now that we have our training component, we can run it. We begin by generating a unique name for the mode;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad76242b",
      "metadata": {
        "gather": {
          "logged": 1671089220135
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "model_name_suffix = int(time.time())\n",
        "model_name = \"rai_programmer_example_reg\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49615a7",
      "metadata": {},
      "source": [
        "Next, we define our training pipeline. This has two components. The first is the training component which we defined above. The second is a component to register the model in AzureML:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6c6cec",
      "metadata": {
        "gather": {
          "logged": 1671089220385
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import dsl, Input\n",
        "\n",
        "programmers_train_mltable = Input(\n",
        "    type=\"mltable\",\n",
        "    path=f\"azureml:{input_train_data}:{rai_programmer_example_version_string}\",\n",
        "    mode=\"download\",\n",
        ")\n",
        "programmers_test_mltable = Input(\n",
        "    type=\"mltable\",\n",
        "    path=f\"azureml:{input_test_data}:{rai_programmer_example_version_string}\",\n",
        "    mode=\"download\",\n",
        ")\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=compute_name,\n",
        "    description=\"Register Model for RAI Programmers example\",\n",
        "    experiment_name=f\"RAI_Programmers_Example_Model_Training_{model_name_suffix}\",\n",
        ")\n",
        "def my_training_pipeline(target_column_name, training_data):\n",
        "    trained_model = train_model_component(\n",
        "        target_column_name=target_column_name, training_data=training_data\n",
        "    )\n",
        "    trained_model.set_limits(timeout=120)\n",
        "\n",
        "    _ = register_component(\n",
        "        model_input_path=trained_model.outputs.model_output,\n",
        "        model_base_name=model_name,\n",
        "        model_name_suffix=model_name_suffix,\n",
        "    )\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "model_registration_pipeline_job = my_training_pipeline(\n",
        "    target_column_name, programmers_train_mltable\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa66ea6",
      "metadata": {},
      "source": [
        "With the training pipeline defined, we can submit it for execution in AzureML. We define a helper function to wait for the job to complete:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f854eef5",
      "metadata": {
        "gather": {
          "logged": 1671089139482
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import PipelineJob\n",
        "from IPython.core.display import HTML\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
        "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
        "    assert created_job is not None\n",
        "\n",
        "    print(\"Pipeline job can be accessed in the following URL:\")\n",
        "    display(HTML('<a href=\"{0}\">{0}</a>'.format(created_job.studio_url)))\n",
        "\n",
        "    while created_job.status not in [\n",
        "        \"Completed\",\n",
        "        \"Failed\",\n",
        "        \"Canceled\",\n",
        "        \"NotResponding\",\n",
        "    ]:\n",
        "        time.sleep(30)\n",
        "        created_job = ml_client.jobs.get(created_job.name)\n",
        "        print(\"Latest status : {0}\".format(created_job.status))\n",
        "    assert created_job.status == \"Completed\"\n",
        "    return created_job\n",
        "\n",
        "\n",
        "# This is the actual submission\n",
        "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "13c594f5",
      "metadata": {},
      "source": [
        "## Create Responsible AI Insight Dashboard\n",
        "\n",
        "Now, we will create a job within the Azure ML Studio with the no-code wizard experience. Since we already registered our model, we can now generate Responsible AI Insights wih the \"**Responsible AI > Create dashboard**\" button.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard.png)\n",
        "\n",
        "First, we need to pick a train and test dataset that we used to train and test your model.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-train-dataset.png)\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-test-dataset.png)\n",
        "\n",
        "Next, we will be chossing regression to match the model.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-regression-task.png)\n",
        "\n",
        "For this scenario, we have a choice of chossing either the debugging profile or real-life intervention profile. For now, let's choose the **Model Debugging** profile.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-regression-debug.png)\n",
        "\n",
        "Let's move forward with model debugging and customize the dashboard to include error analysis, counterfactual analysis, and model explanation. \n",
        "\n",
        "For the error analysis, we can select up to two features to pre-generate an error heat map for.\n",
        "\n",
        "For the counterfactual analysis, let's investigate some examples (let's go with 10example per datapoint) where we automatically exercise features just enough, so we can get a score between 7 and 10. We can see which features are perturb if we don't want certain features to be changed.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-regression-parameters-debugging.png)\n",
        "\n",
        "Once satisfied, we can now move on to the final step to configure our experiment. From here, we can provide a job name for our Responsible AI dashboard.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-regression-exp.png)\n",
        "Now, we wait for the job to be completed.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-regression-exp-pending.png)\n",
        "\n",
        "We can also check on the job's progress by clicking the \"**View Job**\" button.\n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-regression-exp-progress.png)\n",
        "\n",
        "## Viewing the RAI Dashboard\n",
        "\n",
        "We can now view the dashboard(s) by navigating to the registered model we've just created. This can be accomplished by clicking on the **Responsible AI** tab at the model's detail page. \n",
        "\n",
        "An alternative is to click on the **View** button after the job is completed. \n",
        "\n",
        "![Alt text](Media/create-rai-dashboard-regression-exp-completed.png)\n",
        "\n",
        "We have enabled an intergration of our workspace compute resources to access all the features such as retraining error trees, recalculating probabilities and generating insights in real time. \n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-insights.png)\n",
        "\n",
        "The different components of the Responsible AI dashboard are designed such that they can easily communicate with each other. You can create cohorts of your data to slice and dice your analysis and interactively pass cohorts and insights from one component to another for deep-dive investigations. You can hide the different components you’ve generated for the dashboard in the “dashboard configuration” or add them back by clicking the blue “plus” icon.\n",
        "\n",
        "We first look at our error tree, which tells us where the distribution of most of our errors lie. It seems that our models made the greatest number of errors for programmers living in Antarctica who don’t program in C, PHP, or Swift and don’t contribute that often to GitHub repos. We can easily save this as a new cohort to investigate later, but in the meanwhile it will show up as a “Temporary cohort” in the subsequent components.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-insights-generate2.png)\n",
        "\n",
        "We can use the **data analysis** to see if feature distribution in our dataset is skewed. This can cause a model to incorrectly predict datapoints belonging to an underrepresented group or to be optimized along an inappropriate metric. If we bin our x-axis to be the ground truth of different scores a programmer can get (where 7-10 is the accepted range) and look at the style, we see that there is a highly skewed distribution of programmers who use tabs being scored lower and programmers who use spaces being scored higher.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-insights-data-analysis-2.png)\n",
        "\n",
        "Since we know our model made the most amount of error for those living in Antarctica, when we investigate location, we see a highly skewed distribution of programmers living in Antarctica who were scored lower. What this means is that our model will unfairly favor those who are using spaces, and not living in Antarctica when providing access to the application we built.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-insights-data-analysis-location.png)\n",
        "\n",
        " For **feature importance**, we can see for our overall model, which features were the most important to the model’s predictions; and we can see that style (tabs or spaces) is by far the most considered, then operating system then programming language. If we click into style, we can see that using ‘spaces’ has a positive feature importance and ‘tabs’ has a negative feature importance showing us that ‘spaces’ is what contributes to a higher score.\n",
        "\n",
        " ![Alt text](Media/rai-dashboard-regression-insights-feature.png)\n",
        "\n",
        "We can also look at two specific programmers who got a low and high score. **Row 35** has a high score and uses spaces and **row 2** has a low score and uses tabs. When we look at the individual feature importance of each programmers’ features, we can see that the ‘spaces’ positively contributed to **Row 35**’s high score, while ‘tabs’ contributed negatively towards a lower score for **Row 2**. \n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-insights-individual-feature.png)\n",
        "\n",
        "We can take a deeper look with counterfactual what-if examples. When selecting someone below the 7 to 10 range prediction, we can see what bare minimum changes could happen to their features to lead to much higher predictions. In this programmer’s case, some recommended changes would be switching their style to spaces.\n",
        "\n",
        "Let's select **Index 440** and create a what-if counterfactual.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-insights-counterfactuals.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-insights-counterfactuals-2.png)\n",
        "\n",
        "Finally, if we wanted to purely use historic data to identify the features that have the most direct effect on our outcome of interest, in this case the score, we can use causal analysis.  \n",
        "\n",
        "For that, we create a new Responsible Dashboard with **Real-Life Interventions** components.\n",
        "To create a new dashboard, go to the same model and under Responsible AI, click on Responsible AI insights, click on Create Dashboard \n",
        "\n",
        "![Alt text](Media/responsible-ai-insights.png)\n",
        "\n",
        "Follow the same steps followed before until where you can choose Real Life Interventions\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-real-life-interventions.png)\n",
        "\n",
        "In our case, we want to understand the causal effect of years of experience and number of GitHub repos a programmer has contributed to on the score. \n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-real-life-interventions-2.png)\n",
        "\n",
        "Be sure to use the same experiment for tracking purposes.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-real-life-interventions-exp.png)\n",
        "\n",
        "The aggregate causal effects show you overall for your whole dataset, on average, increasing the number of GitHub repos by 1 increases the score by 0.096 whereas increasing the number of years of experience by 1 doesn’t increase the score by much at all.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-real-life-interventions-causal-analysis.png)\n",
        "\n",
        "However, if we want to look at individual programmers and perturb those values and see the outcome of specific treatments to years of experience, we can see that for some programmers, increasing the years of experience does cause the score to increase by a bit.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-real-life-interventions-causal-analysis-2.png)\n",
        "\n",
        "Additionally, the treatment policy tab can help us decide what overall treatment policy to take to maximize real-world impact on our score.  We can see the best future interventions to apply to certain segmentations of our programmer population to see the biggest boost in the scores overall.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-real-life-interventions-causal-analysis-3.png)\n",
        "\n",
        "And if you can only focus on 10 programmers to reach out to, you can see a ranked list of top k programmers who would gain the most from either increasing or decreasing the number of GitHub repos.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-real-life-interventions-causal-analysis-4.png)\n",
        "\n",
        "## Create a RAI Scorecard with UI\n",
        "\n",
        "You can create a scorecard for any Responsible AI dashboard you generate by clicking on **Create Responsible AI insights > Generate new PDF scorecard** which will open up a panel for you to walk through the same steps via a UI wizard but without any code.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-2.1.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-2.1.1.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-3.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-4.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-4.1.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-5.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-6.png)\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-7.1.png)\n",
        "\n",
        "## Download the Responsible AI scorecard\n",
        "\n",
        "To view your Responsible AI scorecard, go into your model registry and select the registered model you have generated a Responsible AI dashboard for. Once you click on your model, click on the **Responsible AI** tab to view a list of generated dashboards. Select which dashboard you’d like to export a Responsible AI scorecard PDF for by clicking **Create Responsible AI insights > View all PDF scorecards**.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-pre-final.png)\n",
        "\n",
        "Select which scorecard you’d like to download from the list and click Download to download the PDF to your machine.\n",
        "\n",
        "![Alt text](Media/rai-dashboard-regression-PDF-scorecard-generate-final.png)\n",
        "\n",
        "## Learn how to read the Responsible AI scorecard\n",
        "\n",
        "The Responsible AI scorecard is a PDF summary of your key insights from the Responsible AI dashboard. The first summary segment of the scorecard gives you an overview of the machine learning model and the key target values you have set to help all stakeholders determine if your model is ready to be deployed.\n",
        "\n",
        "![Alt text](Media/rai-scorecard-pdf.png)\n",
        "\n",
        "The data explorer segment shows you characteristics of your data, as any model story is incomplete without the right understanding of data.\n",
        "\n",
        "![Alt text](Media/rai-scorecard-pdf-data.png)\n",
        "\n",
        "The model performance segment displays your model’s most important metrics and characteristics of your predictions and how well they satisfy your desired target values.\n",
        "\n",
        "![Alt text](Media/rai-scorecard-pdf-performance.png)\n",
        "\n",
        "Next, you can also view the top performing and worst performing data cohorts and subgroups that are automatically extracted for you to see the blind spots of your model.\n",
        "\n",
        "![Alt text](Media/rai-scorecard-pdf-cohorts.2.png)\n",
        "\n",
        "Then you can see the top important factors impacting your model predictions, which is a requirement to build trust with how your model is performing its task. You can further see your model fairness insights summarized and inspect how well your model is satisfying the fairness target values you had set for your desired sensitive groups.\n",
        "\n",
        "![Alt text](Media/rai-scorecard-pdf-features.png)\n",
        "\n",
        "![Alt text](Media/rai-scorecard-pdf-features-2.png)\n",
        "\n",
        "![Alt text](Media/rai-scorecard-pdf-features-2.1.png)\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Currenlty, the Responsible AI dashboard in Azure Machine Learning, is generatable via a variety of experiences through CLI, SDK, or a no-code UI experience. You can also generate the **Responsible AI scorecard**, a PDF report you can extract and share with summaries of key data and model performance and fairness insights. The dashboard supports the ML professional personas, empowering them to easily debug and improve their machine learning models, while the scorecard helps technical and non-technical audiences alike understand the impact of applying Responsible AI, so that all stakeholders can participate in compliance reviews."
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "sdk",
      "python",
      "responsible-ai"
    ],
    "celltoolbar": "Raw Cell Format",
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd2f20a2ae7e9e927b52643942994f3aab4e8a0fff0d99512b6bf37211656242"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
